# Quick test configuration - runs for ~10 seconds then exits
model:
  n_layers: 2  # Smaller model
  input_dim: 1
  target_dim: 1
  pool: "none"
  
  block:
    use_rms_norm: true
    
    attn:
      n_heads: 2  # Fewer heads
      causal: true
      use_rope: true
      use_rms_norm: true
      init: "scaled_xavier"
      rope_max_seq_len: 64  # Shorter sequences
      
    moe:
      d_model: 64  # Much smaller model
      
      router:
        n_experts: 4   # Fewer experts
        k: 2
        use_rms_norm: true
        load_balance_weight: 0.01
        router_type: "topk"
        
      expert:
        d_model: 64
        d_hidden: 128   # Much smaller
        activation: "swiglu"
        init: "scaled_xavier"
        dropout: 0.1

training:
  batch_size: 32  # Smaller batches
  learning_rate: 0.001
  weight_decay: 0.01
  warmup_steps: 10
  max_steps: 100    # Just 50 steps for quick testing!
  scheduler: "cosine_with_warmup"
  gradient_clip_norm: 1.0
  eval_interval: 20  # Eval twice during test
  save_interval: 25
  
  gradient_accumulation_steps: 1
  mixed_precision: false

dataset:
  type: "sine"  # Simple sine waves only
  sequence_length: 32  # Shorter sequences
  prediction_length: 8   # Shorter predictions
  num_samples: 1000      # Much fewer samples
  sampling_rate: 100.0
  
optimizer:
  type: "adamw"
  betas: [0.9, 0.95]
  eps: 1e-8
