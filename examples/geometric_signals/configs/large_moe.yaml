# Large MoE configuration for geometric signal learning
model:
  n_layers: 8
  input_dim: 1
  target_dim: 1
  
  block:
    use_rms_norm: true
    
    attn:
      n_heads: 8
      causal: true
      use_rope: true
      use_rms_norm: true
      init: "scaled_xavier"
      rope_max_seq_len: 256
      
    moe:
      d_model: 512  # Larger model dimension
      
      router:
        n_experts: 32  # Many more experts for specialization
        k: 4           # Select top-4 experts
        use_rms_norm: true
        load_balance_weight: 0.01
        router_type: "topk"
        capacity_factor: 1.25  # Allow more capacity
        
      expert:
        d_model: 512
        d_hidden: 2048     # 4x expansion in FFN
        activation: "swiglu"
        init: "scaled_xavier"
        dropout: 0.1

training:
  batch_size: 32  # Smaller batch due to larger model
  learning_rate: 0.0005
  weight_decay: 0.01
  warmup_steps: 2000
  max_steps: 100000
  scheduler: "cosine_with_warmup"
  gradient_clip_norm: 1.0
  eval_interval: 2000
  save_interval: 10000
  
  gradient_accumulation_steps: 4  # Larger effective batch
  mixed_precision: false

dataset:
  type: "mixed"
  sequence_length: 128
  prediction_length: 32
  num_samples: 200000  # More data for larger model
  sampling_rate: 100.0
  
optimizer:
  type: "adamw"
  betas: [0.9, 0.95]
  eps: 1e-8